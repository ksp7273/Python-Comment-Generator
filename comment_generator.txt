{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code LLaMA-Based Comment Generator for Python Code\n",
        "\n",
        "This notebook implements a tool to generate explanatory comments for Python code snippets using a fine-tuned Code LLaMA model, served via a FastAPI endpoint. It runs in Google Colab with GPU support.\n",
        "\n",
        "## Steps\n",
        "1. Install dependencies and download the CodeSearchNet dataset.\n",
        "2. Fine-tune Code LLaMA for comment generation.\n",
        "3. Create a FastAPI app to accept code snippets and return commented code.\n",
        "4. Run the API in Colab using pyngrok for public access.\n",
        "\n",
        "**Note**: Requires a GPU runtime (T4 or better). Go to Runtime > Change runtime type > Select GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies and Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets transformers peft accelerate torch fastapi uvicorn python-multipart pyngrok\n",
        "\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Download CodeSearchNet Python subset\n",
        "dataset = load_dataset(\"code_search_net\", \"python\")\n",
        "\n",
        "# Select a small subset (10,000 samples) for fine-tuning\n",
        "small_dataset = dataset['train'].shuffle(seed=42).select(range(10000))\n",
        "\n",
        "# Save to disk\n",
        "output_dir = \"/content/code_search_net_python_subset\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "small_dataset.save_to_disk(output_dir)\n",
        "\n",
        "print(f\"Dataset saved to {output_dir}\")\n",
        "# Example: View one sample\n",
        "print(small_dataset[0]['func_code_string'])  # Code\n",
        "print(small_dataset[0]['func_documentation_string'])  # Comment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fine-Tune Code LLaMA\n",
        "\n",
        "Fine-tune CodeLlama-7b-Python-hf using LoRA for efficiency. This step assumes a GPU is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_from_disk(\"/content/code_search_net_python_subset\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"codellama/CodeLlama-7b-Python-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "\n",
        "# Add padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess(examples):\n",
        "    prompts = [\n",
        "        f\"Generate explanatory comments for this Python code:\\n{code}\\n### Comments:\\n{comment}\"\n",
        "        for code, comment in zip(examples['func_code_string'], examples['func_documentation_string'])\n",
        "    ]\n",
        "    return tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/fine_tuned_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    weight_decay=0.001,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"linear\",\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"/content/fine_tuned_code_llama\")\n",
        "tokenizer.save_pretrained(\"/content/fine_tuned_code_llama\")\n",
        "print(\"Fine-tuning complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create FastAPI Endpoint\n",
        "\n",
        "Create a FastAPI app to accept code snippets (text or file) and return commented code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, UploadFile, File, Form\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import ast\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load fine-tuned model\n",
        "model_dir = \"/content/fine_tuned_code_llama\"  # Or \"codellama/CodeLlama-7b-Python-hf\" if not fine-tuned\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.float16)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "def generate_commented_code(code: str) -> str:\n",
        "    prompt = f\"Add explanatory comments beside the code lines in this Python snippet:\\n{code}\\n### Commented code:\\n\"\n",
        "    generated = generator(prompt, max_new_tokens=300, num_return_sequences=1, temperature=0.7)[0]['generated_text']\n",
        "    if \"### Commented code:\\n\" in generated:\n",
        "        return generated.split(\"### Commented code:\\n\")[-1].strip()\n",
        "    return generated\n",
        "\n",
        "@app.post(\"/generate_comments\")\n",
        "async def add_comments(code_file: UploadFile = File(None), code_text: str = Form(None)):\n",
        "    if code_file:\n",
        "        code = (await code_file.read()).decode(\"utf-8\")\n",
        "    elif code_text:\n",
        "        code = code_text\n",
        "    else:\n",
        "        return {\"error\": \"Provide code via file upload or text.\"}\n",
        "    \n",
        "    try:\n",
        "        ast.parse(code)\n",
        "    except SyntaxError:\n",
        "        return {\"error\": \"Invalid Python code.\"}\n",
        "    \n",
        "    commented_code = generate_commented_code(code)\n",
        "    return {\"original_code\": code, \"commented_code\": commented_code}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run FastAPI in Colab with pyngrok\n",
        "\n",
        "Use pyngrok to expose the FastAPI app publicly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Set up ngrok (get your authtoken from https://dashboard.ngrok.com)\n",
        "!ngrok authtoken YOUR_NGROK_AUTH_TOKEN  # Replace with your token\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run FastAPI in a background thread\n",
        "def run_api():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "threading.Thread(target=run_api, daemon=True).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Test the API\n",
        "\n",
        "Test the endpoint using curl or Python requests. Example using requests:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "url = \"<your_ngrok_url>/generate_comments\"  # Replace with the ngrok URL from above\n",
        "code = \"def add(a, b): return a + b\"\n",
        "response = requests.post(url, data={\"code_text\": code})\n",
        "print(response.json())\n",
        "```\n",
        "\n",
        "Or test with a file:\n",
        "\n",
        "```bash\n",
        "curl -X POST \"<your_ngrok_url>/generate_comments\" -F \"code_text=def add(a, b): return a + b\"\n",
        "```\n",
        "\n",
        "**Note**: Fine-tuning takes 2-4 hours on a T4 GPU. For testing without fine-tuning, use the pre-trained model by setting `model_dir = \"codellama/CodeLlama-7b-Python-hf\"`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}